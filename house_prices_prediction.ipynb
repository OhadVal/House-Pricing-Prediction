{
  "cells": [
    {
      "metadata": {
        "_uuid": "4c348eb27fe3977713ceb0a0f02d2351b2b0aef3"
      },
      "cell_type": "markdown",
      "source": "**Introduction**\n\nThe following dataset shows a history of house sales in Ames, Iowa.\n\nWe want to be able to predict a house price based on the information in the given dataset."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#invite friends for the Kaggle party\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ",
      "execution_count": 76,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3a958d32ac697c84d4b600feb62ee37ad3c65dd0"
      },
      "cell_type": "markdown",
      "source": "Know your data - exploratory data analysis!"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n#starting with EDA\ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d2cb3bf6cfd8b9094ead194ecfdf829eaa1a5ff"
      },
      "cell_type": "code",
      "source": "train.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cd3a3edc466e0b28f6218d51023d8fa2026aac3"
      },
      "cell_type": "code",
      "source": "train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85e80bc0704e49272a3a2cbd85a43b75712ad03a"
      },
      "cell_type": "code",
      "source": "train.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "471338d10588b2b091fc92d7f0e606822c1a888f"
      },
      "cell_type": "markdown",
      "source": "There are a lot of columns to work with, let's check which do we need.\n\nWe'll begin by inspecting which columns correlate best with 'SalePrice'."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14df9d3fa8d79bc971a129adde84551c88976a81"
      },
      "cell_type": "code",
      "source": "#SalePrice correlation matrix\ncorrelations = train.corr()\ncols = correlations.nlargest(10, 'SalePrice').index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show() #display heatmap\n\n#take the 5 columns in which the correlation is highest.\ncorrelations = correlations[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[1:6]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0aff6fb7b5191ce40e653e21d62cfacba53d8d68"
      },
      "cell_type": "markdown",
      "source": "**Handling Missing Data:**\n\nFor a dataset of this size, a lot of missing values can be found. In order to effectively train our model we build, we must first deal with the missing values. There are missing values for both numerical and categorical data. \n\nFor numerical imputing, we would typically fill the missing values with a measure like median, mean, or mode.\nFor categorical imputing, we'll fill the missing values with the most common term that appeared from the entire column (one of many techniques). "
    },
    {
      "metadata": {
        "_uuid": "7a95988cf93ff96a1695fb9f7c1250ebcd38602d"
      },
      "cell_type": "markdown",
      "source": "We can see in the data description file, that for some categories, NaN means something.\nThis means that if a value is NaN, the house might not have that certain attribute, which will affect the price of the house.\nWe will deal with it by filling the null cell with \"None\"."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b25d92f1e9cf199de7603fda4aa4f9ce58fb477d"
      },
      "cell_type": "code",
      "source": "train_null = pd.isnull(train).sum() #number of null values for each column in the train set\ntest_null = pd.isnull(test).sum() #number of null values for each column in the test set\n\nnull = pd.concat([train_null, test_null], axis=1, keys=[\"Train\", \"Test\"], sort='True')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c176f0c62eb1ed480a4f27104a639cea9f40325d"
      },
      "cell_type": "code",
      "source": "null_many = null[null.sum(axis=1) > 200]  #many missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few much missing values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e443105db6c59f0c283f87e49e25c267c3a1961d"
      },
      "cell_type": "code",
      "source": "null_many",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b535d362afb6acfdaf1efcb615c1edfe388eb063"
      },
      "cell_type": "markdown",
      "source": "For example, we can see that there are a lot of missing values in the 'Alley' column. A quick look at the description will show us that NaN in the 'Alley' column stands for no alley access."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ad40cf00733f15bd11c0afad45ee746e0fe50cb"
      },
      "cell_type": "code",
      "source": "#more can be found on the description data file provided\n\nnull_has_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0831180279932deed0e80fbea454326c14970ff0"
      },
      "cell_type": "code",
      "source": "#change the null value to \"None\" where null means something\nfor null_value in null_has_meaning:\n    train[null_value].fillna(\"None\", inplace=True)\n    test[null_value].fillna(\"None\", inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5dc4bee5646d687cb03cc2ae598c9c7c1156cf5c"
      },
      "cell_type": "markdown",
      "source": "Dealing with the \"real\" NaN values:\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17ff7b4274e22254447827b4bf68167cb3f90ffa"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import Imputer\n\nimputer = Imputer(strategy=\"median\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce758dc0081ae028a31688cc80316dc7a1e96823"
      },
      "cell_type": "markdown",
      "source": "We made some changes to null values, so let's update our dataframes:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2a47eef19ed0ef47d80df9367e4d0ce4cf8d23f"
      },
      "cell_type": "code",
      "source": "train_null = pd.isnull(train).sum() #number of null values for each column in the train set\ntest_null = pd.isnull(test).sum() #number of null values for each column in the test set\n\nnull = pd.concat([train_null, test_null], axis=1, keys=[\"Train\", \"Test\"], sort='True')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "059edcc6faac05032f67fb6a3bf779f049009d7a"
      },
      "cell_type": "code",
      "source": "null_many = null[null.sum(axis=1) > 200]  #many missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few much missing values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9c25869335a1a1b4a8f02fa57bc0c89fc4857d8"
      },
      "cell_type": "code",
      "source": "null_many",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0127ad9f7a534cdfb51980fd581db2d8a07046b8"
      },
      "cell_type": "markdown",
      "source": "It seems like 'LotFrontage' has too many null values and it is a numerical value so it may be better to drop it."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40b71ec436c401e6a223c880a3ba52f7850e855e"
      },
      "cell_type": "code",
      "source": "train.drop(\"LotFrontage\", axis=1, inplace=True)\ntest.drop(\"LotFrontage\", axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a27e5e603101d165ebb27dd65c704b5c7dbb41e0"
      },
      "cell_type": "code",
      "source": "null_few",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6564edc34831479ac7bb880c09d7a9a79fdb6fac"
      },
      "cell_type": "markdown",
      "source": "GarageYrBlt, MasVnrArea, and MasVnrType all have a decent amount of missing values. MasVnrType is categorical so we can replace the missing values with \"None\", as we did before. We'll fill the others with median."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "520783a99448612e56ea73e57ffbd26a68607a4c"
      },
      "cell_type": "code",
      "source": "train[\"GarageYrBlt\"].fillna(train[\"GarageYrBlt\"].median(), inplace=True)\ntest[\"GarageYrBlt\"].fillna(test[\"GarageYrBlt\"].median(), inplace=True)\ntrain[\"MasVnrArea\"].fillna(train[\"MasVnrArea\"].median(), inplace=True)\ntest[\"MasVnrArea\"].fillna(test[\"MasVnrArea\"].median(), inplace=True)\ntrain[\"MasVnrType\"].fillna(\"None\", inplace=True)\ntest[\"MasVnrType\"].fillna(\"None\", inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7a62e943c175d8b1ffc2e4553446e8fb23af58cb"
      },
      "cell_type": "markdown",
      "source": "We took care of the features with a lot of missing values, now we'll take care of the ones with few missing values."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ae6ae320df7c7fb19e954a8fc499bc0b30ec7ff"
      },
      "cell_type": "code",
      "source": "#split to numerical (type = int, float) and categorical (type = object) featues:\n\n#train set\ntypes_train = train.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] \ncat_train = types_train[types_train == object] \n\n#test set\ntypes_test = test.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e52a3f9039434ef407796f06425be6ac87811839"
      },
      "cell_type": "markdown",
      "source": "Numerical Imputing"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0790ddaa54765754572d36b0b4cdbedd9301e8a7"
      },
      "cell_type": "code",
      "source": "sns.distplot(train['SalePrice'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f5706e2d923bf0ba06b8d151e81be7617d6bd1d"
      },
      "cell_type": "markdown",
      "source": "We can see that our data is skewed right, so we'll impute with median."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86ee5b888b046d73691888abfc40789744fffd92"
      },
      "cell_type": "code",
      "source": "#lists are easier to work with, so we'll convert num_train and num_test.\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58cc8759cb2557607a73520f17146b7474a8cbbd"
      },
      "cell_type": "markdown",
      "source": "Those are all of the numerical features in our data:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e8dcd500f9594ce317cdb1bb6647d35dda187cf"
      },
      "cell_type": "code",
      "source": "print (numerical_values_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9d3e5e61a0af43c57dcef156fb0b9348f573115"
      },
      "cell_type": "code",
      "source": "#create a list of all features with missing values\nmissing_num = []\n\nfor feature in numerical_values_train:\n    if feature in list(null_few.index):\n        missing_num.append(feature)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "92045fc6cfe106e2ca956849471ead92a08d0f95"
      },
      "cell_type": "markdown",
      "source": "Those are all of the numerical features with missing data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7f0eb7b50ab03db5b24f5577aef46c6cf4514b6"
      },
      "cell_type": "code",
      "source": "print (missing_num)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d923e87e3134195d42ef5b98489e479c604fd07"
      },
      "cell_type": "code",
      "source": "#impute\nfor feature in missing_num:\n    train[feature].fillna(train[feature].median(), inplace=True)\n    test[feature].fillna(test[feature].median(), inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d91eb57f986fd9905b0f6e6bab3212cf8e5a2685"
      },
      "cell_type": "markdown",
      "source": "Categorical Imputing\n\nThose are non-numerical features so we can't use a technique like median value on them. Instead we'll impute with the most common term that appears in the entire list."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07fba58d732a6da351bdda0dabd27bf775514591"
      },
      "cell_type": "code",
      "source": "categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ca1a4e24417e26837b6fba1e1bc3a461b5a0871"
      },
      "cell_type": "markdown",
      "source": "All of the categorical features:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6b38f96a2307415aa5287393e32776e4433f82d"
      },
      "cell_type": "code",
      "source": "print(categorical_values_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4abc49f6399e9608f9a7e0024c87a96bdf3fec75"
      },
      "cell_type": "code",
      "source": "#create a list of all features with missing values\nmissing_cat = []\n\nfor feature in categorical_values_train:\n    if feature in list(null_few.index):\n        missing_cat.append(feature)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c6940a28d91c5de2bf8171b42689d82580d10d07"
      },
      "cell_type": "markdown",
      "source": "Those are all of the categorical features with missing data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "271850a33ff8ff07535bf7d24f7658f1bfba4f2a"
      },
      "cell_type": "code",
      "source": "print(missing_cat)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e44e8b97ec8b42d9a9fe9285cf36334103c08091"
      },
      "cell_type": "code",
      "source": "def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = [\"Electrical\", \"Exterior1st\", \"Exterior2nd\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\", \"MasVnrType\"]\n\ncounter = 0\nfor i in missing_cat:\n    most_common[counter] = most_common_term(train[i])\n    counter += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2f6ebda924d9cb8294471889aaeea3e26489252"
      },
      "cell_type": "markdown",
      "source": "Those are the categorical features with missing values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b46686900a36a9d1b6c97a47724f2408cbca1596"
      },
      "cell_type": "code",
      "source": "most_common_dictionary = {missing_cat[x]: [most_common[x]] for x in range(len(most_common))}\nmost_common_dictionary",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b9ddac9e81008dc9f5974b13c0fbbe25ce4944a"
      },
      "cell_type": "code",
      "source": "#replace null values with most common term\ncounter = 0\nfor feature in missing_cat:  \n    train[feature].fillna(most_common[counter], inplace=True)\n    test[feature].fillna(most_common[counter], inplace=True)\n    counter += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c047f0d70a7b89fc09a0fc7db0d8b92ec3564d7"
      },
      "cell_type": "markdown",
      "source": "We took care of both the numerical features and the categorical featues, if all worked according to our plan we shouldn't have any null values left. Since we are being thorough, we will check if all is well."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37502a1de39ac27f82a8b9ecc1e121080ae16b69"
      },
      "cell_type": "code",
      "source": "#updating the null values series\ntrain_null = pd.isnull(train).sum() #number of null values for each column in the train set\ntest_null = pd.isnull(test).sum() #number of null values for each column in the test set\n\nnull = pd.concat([train_null, test_null], axis=1, keys=[\"Train\", \"Test\"], sort='True')\nnull[null.sum(axis=1) > 0] #all features with 1 or more null values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d15da3ead6b800498be02d54250e1ad7c38021d2"
      },
      "cell_type": "markdown",
      "source": "An empty table, we did it!"
    },
    {
      "metadata": {
        "_uuid": "46276b4901bd206cb5db4538ae2b360de527522b"
      },
      "cell_type": "markdown",
      "source": "**Feature Engineering**"
    },
    {
      "metadata": {
        "_uuid": "46265140a5b0d80e6fc03ab52746087f11afbaa6"
      },
      "cell_type": "markdown",
      "source": "We have dealt with all of the missing values, now it's time for the next step of our data preprocessing - feature engineering!\nWe need to create feature vectors in order to get the data ready for our model as training data. To do so, we will have to convert the categorical values into representative numbers.\n\nAs we saw earlier, out data is skewed right, so we'll use log transformation on it."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8f222a4f13538f4f9164cb67b62e7499608b84d"
      },
      "cell_type": "code",
      "source": "#before log transforamtion\nsns.distplot(train['SalePrice'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8bcb4b7dd5169c30174c97fe12f8d07275aa978f"
      },
      "cell_type": "code",
      "source": "train[\"TransformedPrice\"] = np.log(train[\"SalePrice\"])\n\n#after log transformation\nsns.distplot(np.log(train[\"SalePrice\"]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "606376d99854512f5274d1fddb79d48c69a7a758"
      },
      "cell_type": "markdown",
      "source": "Our target feature SalePrice used to be very skewed, but thanks to the logarithm transformation it is no more.\nNow we can see that it is more normally distributed, which works better with machine learning models.\n\nNow we'll look on the catergorical data that needs to be transformed"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ffc1e71f1e09b340b12440e429b0c030b108fb4"
      },
      "cell_type": "code",
      "source": "categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)\n\nprint(categorical_values_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7060021ae78f27f594be34dc90b9a8007f7c3ad0"
      },
      "cell_type": "code",
      "source": "#convert categorical values into representative numbers\n#train set\nfor feature in categorical_values_train:\n    feature_set = set(train[feature]) #unique values for the feature\n    for cat_val in feature_set:\n        feature_list = list(feature_set)\n        train.loc[train[feature] == cat_val, feature] = feature_list.index(cat_val)\n        \n#test set\nfor feature in categorical_values_test:\n    feature_set2 = set(test[feature]) #unique values for the feature\n    for cat_val in feature_set2:\n        feature_list2 = list(feature_set2)\n        test.loc[test[feature] == cat_val, feature] = feature_list2.index(cat_val)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6d5e12effbee2ce852673de08119c22de233113"
      },
      "cell_type": "code",
      "source": "train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9686ebfaa80f582f96181d8c85988e71d7bbf4b"
      },
      "cell_type": "code",
      "source": "test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "38ef9ade18e917fa3d6fb98931841253313da90f"
      },
      "cell_type": "markdown",
      "source": "Looks like we have changed all the categorical strings into a representative number. Now we can move on to the next step.\n"
    },
    {
      "metadata": {
        "_uuid": "490a8bf2183805037a2d2d5c7406856481ce7739"
      },
      "cell_type": "markdown",
      "source": "**Creating, Training, Evaluating, Validating, and Testing ML Models**\n\nWe've finished the preprocessing part. Now we know and understand our data much better!\nWe can start to build and test different models for regression to predit the sale price of each house.\nWe'll import the models, train them and evaluate them. We'll use the R^2 score and the RMSE to  evaluate our model performance.\nWe will also use cross validation to optimize our model hyperparameters."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c91786fb78e2a1fbe85adc9d3f6c45a6b6ccb26"
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d0e0a800e3246c86e91da9b76eb9f2077cc3a9ee"
      },
      "cell_type": "markdown",
      "source": "**Defining Training/Test Sets**\n\nWe drop the Id and SalePrice columns for the training set since those are not involved in predicting the Sale Price of a house. The SalePrice column will become our training target. Remember how we transformed SalePrice to make the distribution more normal? Well we can apply that here and make TransformedPrice the target instead of SalePrice. This will improve model performance and yield a much smaller RMSE because of the scale."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f757ba6dab0122b720f643bb0f3f3d2d5320eff6"
      },
      "cell_type": "code",
      "source": "X_train = train.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values\ny_train = train[\"TransformedPrice\"].values\nX_test = test.drop(\"Id\", axis=1).values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "48e1cfca323351ea7521f7ab2df4cfc65603db4f"
      },
      "cell_type": "markdown",
      "source": "**Splitting into Validation**\n\nIt is always good to split our training data again into validation sets. This will help us evaluate our model performance as well as avoid overfitting our model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e140ff3a96a019890f18ffa28a3abfb35217fcb0"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4968962fbd29a6c1119a2e8707d87df7c8b2a02d"
      },
      "cell_type": "markdown",
      "source": "**Linear Regression Model**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "359edb47b6294af6ac3ad0017101c72ebde8e78f"
      },
      "cell_type": "code",
      "source": "linreg = LinearRegression()\nparameters_lin = {\"fit_intercept\" : [True, False], \"normalize\" : [True, False], \"copy_X\" : [True, False]}\ngrid_linreg = GridSearchCV(linreg, parameters_lin, verbose=1 , scoring = \"r2\")\ngrid_linreg.fit(X_training, y_training)\n\nprint(\"Best Linear Regression Model: \" + str(grid_linreg.best_estimator_))\nprint(\"Best Score: \" + str(grid_linreg.best_score_))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bbe47078204fbcadedf45a0d4b2bc74072c60b0f"
      },
      "cell_type": "code",
      "source": "linreg = grid_linreg.best_estimator_\nlinreg.fit(X_training, y_training)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d5463310e937d09988e089af0510f3f6e00c990"
      },
      "cell_type": "code",
      "source": "scores_lin = cross_val_score(linreg, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ebed3c437b230e6f8d791d1193deae9d79c688be"
      },
      "cell_type": "markdown",
      "source": "**Decision Tree Regressor Model**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bdcff2fd1915dde5bb82a6a126d0528b7b7f862"
      },
      "cell_type": "code",
      "source": "dtr = DecisionTreeRegressor()\nparameters_dtr = {\"criterion\" : [\"mse\", \"friedman_mse\", \"mae\"], \"splitter\" : [\"best\", \"random\"], \"min_samples_split\" : [2, 3, 5, 10], \n                  \"max_features\" : [\"auto\", \"log2\"]}\ngrid_dtr = GridSearchCV(dtr, parameters_dtr, verbose=1, scoring=\"r2\")\ngrid_dtr.fit(X_training, y_training)\n\nprint(\"Best DecisionTreeRegressor Model: \" + str(grid_dtr.best_estimator_))\nprint(\"Best Score: \" + str(grid_dtr.best_score_))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6db810389d815b9f9fbf59e56561c0321be42a3"
      },
      "cell_type": "code",
      "source": "dtr = grid_dtr.best_estimator_\ndtr.fit(X_training, y_training)\ndtr_pred = dtr.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58c5cb4e64247f9f001585a0472481ef69d3b093"
      },
      "cell_type": "code",
      "source": "scores_dtr = cross_val_score(dtr, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e50287e65d6cedb1b8abbdbb003085086affa0c2"
      },
      "cell_type": "markdown",
      "source": "**Random Forest Regressor**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc5c59aa8edb13af164600a456aff6fc18d32565"
      },
      "cell_type": "code",
      "source": "rf = RandomForestRegressor()\nparemeters_rf = {\"n_estimators\" : [5, 10, 15, 20], \"criterion\" : [\"mse\" , \"mae\"], \"min_samples_split\" : [2, 3, 5, 10], \n                 \"max_features\" : [\"auto\", \"log2\"]}\ngrid_rf = GridSearchCV(rf, paremeters_rf, verbose=1, scoring=\"r2\")\ngrid_rf.fit(X_training, y_training)\n\nprint(\"Best RandomForestRegressor Model: \" + str(grid_rf.best_estimator_))\nprint(\"Best Score: \" + str(grid_rf.best_score_))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71c7792f99e0e60a4c80ea1125b249a40d0be77c"
      },
      "cell_type": "code",
      "source": "rf = grid_rf.best_estimator_\nrf.fit(X_training, y_training)\nrf_pred = rf.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "869e07ac1ebabfe89b2e2d7c73c3cacf3359d6ea"
      },
      "cell_type": "code",
      "source": "scores_rf = cross_val_score(rf, X_training, y_training, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ecaa91b497f0cb510e78f3652b9ff0137497194b"
      },
      "cell_type": "markdown",
      "source": "**Evaluation Our Models**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4688e586dec8a7774c8aa291836d3d248c2457c5"
      },
      "cell_type": "markdown",
      "source": "We have built and trained a few different regression models, now we'll compare them to see which one is best and should be used to predict on the test test."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5bb7ad0ec6234a1ffc47c62bbea914a8be7d714"
      },
      "cell_type": "code",
      "source": "model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Decision Tree Regressor\", \"Random Forest Regressor\"],\n    \"Best Score\" : [grid_linreg.best_score_, grid_dtr.best_score_, grid_rf.best_score_],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8]]\n})\n\nmodel_performances.round(4)\n\nprint(\"Sorted by Best Score:\")\nmodel_performances.sort_values(by=\"Best Score\", ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14e065c8474e540280234f50e32c3f3cd8a62e81"
      },
      "cell_type": "code",
      "source": "print(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12b7cd07b3a2dcaba282a7050fc6e5b178bd1d59"
      },
      "cell_type": "code",
      "source": "print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bc104f7f3e6da174bcdd09ba647e0cd5e218538f"
      },
      "cell_type": "markdown",
      "source": "The RMSEs are small because of the log transformation we performed. So even a 0.1 RMSE may be significant in this case."
    },
    {
      "metadata": {
        "_uuid": "8c6cb25a0072879f75a5c61ce6eab21287761d96"
      },
      "cell_type": "markdown",
      "source": "I chose to use Random Forest Regressor because it ranked first on 2 of our 3 measurements.\nIt has a low RMSE and a high R^2."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "234b4de36866734466a979f22897eef9987a5101"
      },
      "cell_type": "code",
      "source": "rf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69b7d0ed4089e641cff493bdb131aa003ac79806"
      },
      "cell_type": "code",
      "source": "final_predictions = np.exp(rf.predict(X_test))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c5cff0d1f1f5f8516058f77c73da4fea7ef231cc"
      },
      "cell_type": "code",
      "source": "results = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": final_predictions\n    })\n\nprint(results.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4e38471a8011bba48a8de73eb5950882ea53101"
      },
      "cell_type": "code",
      "source": "results.head(10)",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 75,
          "data": {
            "text/plain": "     Id      SalePrice\n0  1461  125971.779476\n1  1462  157017.488253\n2  1463  168426.399809\n3  1464  208868.713918\n4  1465  176834.384424\n5  1466  196824.640202\n6  1467  169242.191740\n7  1468  175351.270525\n8  1469  186593.337109\n9  1470  140957.780490",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1461</td>\n      <td>125971.779476</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1462</td>\n      <td>157017.488253</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1463</td>\n      <td>168426.399809</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1464</td>\n      <td>208868.713918</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1465</td>\n      <td>176834.384424</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1466</td>\n      <td>196824.640202</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1467</td>\n      <td>169242.191740</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1468</td>\n      <td>175351.270525</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1469</td>\n      <td>186593.337109</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1470</td>\n      <td>140957.780490</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cb431b171aa1c2ed5091fb57337181c760e2a9e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}